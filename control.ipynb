{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      " Patch Collection Done!\n"
     ]
    }
   ],
   "source": [
    "import xlwt\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "def Txt_to_Excel(inputTxt,sheetName,start_row,start_col,outputExcel):\n",
    "    fr = codecs.open(inputTxt,'r')\n",
    "    wb = xlwt.Workbook(encoding = 'utf-8')\n",
    "    ws = wb.add_sheet(sheetName)\n",
    "    \n",
    "\n",
    "    line_number = 0#记录有多少行，相当于写入excel时的i，\n",
    "    row_excel = start_row\n",
    "    try:\n",
    "        for line in fr :\n",
    "            \n",
    "            if re.match('0',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            if re.match('-1',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            line = re.sub('1','',line) \n",
    "            \n",
    "            line_number +=1\n",
    "            row_excel +=1\n",
    "            line = line.strip()\n",
    "            \n",
    "            line = line.split(' ')\n",
    "            \n",
    "            len_line = len(line)#list中每一行有多少个数，相当于写入excel中的j\n",
    "            col_excel = start_col\n",
    "            for j in range(len_line):\n",
    "                #print (line[j])\n",
    "                ws.write(row_excel,col_excel,line[j])\n",
    "                col_excel +=1\n",
    "                wb.save(outputExcel)\n",
    "        print (line_number)\n",
    "    except:\n",
    "        print ('')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    sheetName = 'Sheet2'#需要写入excel中的Sheet2中，可以自己设定\n",
    "    start_row = 0 #从第7行开始写\n",
    "    start_col = 0 #从第3列开始写\n",
    "    inputfile = 'control.txt' #输入文件\n",
    "    outputExcel = 'contrl_one.xls' #输出excel文件\n",
    "    Txt_to_Excel(inputfile,sheetName,start_row,start_col,outputExcel)\n",
    "    print (' Patch Collection Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      " Patch Collection Done!\n"
     ]
    }
   ],
   "source": [
    "import xlwt\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "def Txt_to_Excel(inputTxt,sheetName,start_row,start_col,outputExcel):\n",
    "    fr = codecs.open(inputTxt,'r')\n",
    "    wb = xlwt.Workbook(encoding = 'utf-8')\n",
    "    ws = wb.add_sheet(sheetName)\n",
    "    \n",
    "\n",
    "    line_number = 0#记录有多少行，相当于写入excel时的i，\n",
    "    row_excel = start_row\n",
    "    try:\n",
    "        for line in fr :\n",
    "            \n",
    "            if re.match('1',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            if re.match('0',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            line = re.sub('-1','',line) \n",
    "            \n",
    "            line_number +=1\n",
    "            row_excel +=1\n",
    "            line = line.strip()\n",
    "            \n",
    "            line = line.split(' ')\n",
    "            \n",
    "            len_line = len(line)#list中每一行有多少个数，相当于写入excel中的j\n",
    "            col_excel = start_col\n",
    "            for j in range(len_line):\n",
    "                #print (line[j])\n",
    "                ws.write(row_excel,col_excel,line[j])\n",
    "                col_excel +=1\n",
    "                wb.save(outputExcel)\n",
    "        print (line_number)\n",
    "    except:\n",
    "        print ('')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    sheetName = 'Sheet2'#需要写入excel中的Sheet2中，可以自己设定\n",
    "    start_row = 0 #从第7行开始写\n",
    "    start_col = 0 #从第3列开始写\n",
    "    inputfile = 'control.txt' #输入文件\n",
    "    outputExcel = 'control_minus.xls' #输出excel文件\n",
    "    Txt_to_Excel(inputfile,sheetName,start_row,start_col,outputExcel)\n",
    "    print (' Patch Collection Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      " Patch Collection Done!\n"
     ]
    }
   ],
   "source": [
    "import xlwt\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "def Txt_to_Excel(inputTxt,sheetName,start_row,start_col,outputExcel):\n",
    "    fr = codecs.open(inputTxt,'r')\n",
    "    wb = xlwt.Workbook(encoding = 'utf-8')\n",
    "    ws = wb.add_sheet(sheetName)\n",
    "    \n",
    "\n",
    "    line_number = 0#记录有多少行，相当于写入excel时的i，\n",
    "    row_excel = start_row\n",
    "    try:\n",
    "        for line in fr :\n",
    "            \n",
    "            if re.match('1',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            if re.match('-1',line) is not None:\n",
    "                #print (line)\n",
    "                continue\n",
    "                \n",
    "            line = re.sub('0','',line) \n",
    "            \n",
    "            line_number +=1\n",
    "            row_excel +=1\n",
    "            line = line.strip()\n",
    "            \n",
    "            line = line.split(' ')\n",
    "            \n",
    "            len_line = len(line)#list中每一行有多少个数，相当于写入excel中的j\n",
    "            col_excel = start_col\n",
    "            for j in range(len_line):\n",
    "                #print (line[j])\n",
    "                ws.write(row_excel,col_excel,line[j])\n",
    "                col_excel +=1\n",
    "                wb.save(outputExcel)\n",
    "        print (line_number)\n",
    "    except:\n",
    "        print ('')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    sheetName = 'Sheet2'#需要写入excel中的Sheet2中，可以自己设定\n",
    "    start_row = 0 #从第7行开始写\n",
    "    start_col = 0 #从第3列开始写\n",
    "    inputfile = 'control.txt' #输入文件\n",
    "    outputExcel = 'control_zero.xls' #输出excel文件\n",
    "    Txt_to_Excel(inputfile,sheetName,start_row,start_col,outputExcel)\n",
    "    print (' Patch Collection Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_5/5qnsz12d7_l9k236zvv3k5lr0000gn/T/jieba.cache\n",
      "Loading model cost 3.539 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from importlib import reload\n",
    "import yaml\n",
    "import sys\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "# set parameters:\n",
    "vocab_dim = 100\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "batch_size = 32\n",
    "n_epoch = 4\n",
    "input_length = 100\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "timesteps = 8\n",
    "\n",
    "import xlwt\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "#加载训练文件\n",
    "def loadfile():\n",
    "    neg=pd.read_excel('neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('pos.xls',header=None,index=None)\n",
    "\n",
    "    combined=np.concatenate((pos[0], neg[0]))\n",
    "    y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))\n",
    "\n",
    "    return combined,y\n",
    "\n",
    "#对句子经行分词，并去掉换行符\n",
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')\n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    model.build_vocab(combined)\n",
    "    \n",
    "    model.train(combined,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    model.save('Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    print('n_sumbols = ', n_symbols)\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))#索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    print (x_train.shape,y_train.shape)\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n",
    "\n",
    "\n",
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print ('Defining a Simple Keras Model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    \n",
    "    #'''\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    #'''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    model.add(LSTM(output_dim=50, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    '''\n",
    "    \n",
    "    #http://xiaosheng.me/2017/07/04/article77/\n",
    "    #'''\n",
    "    model.add(LSTM(output_dim=50, return_sequences=True,\n",
    "               input_shape=(timesteps, n_symbols)))  # 返回一个50维向量的序列\n",
    "    model.add(LSTM(output_dim=50, return_sequences=True))  # 返回一个50维向量的序列\n",
    "    model.add(LSTM(output_dim=50))  # 返回一个独立的50维的向量\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #'''\n",
    "    \n",
    "    '''\n",
    "    model.add(LSTM(output_dim=50, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax')) # Dense=>全连接层,输出维度=3\n",
    "    model.add(Activation('softmax'))\n",
    "    '''\n",
    "    \n",
    "    print ('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print (\"Train...\")\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=n_epoch,verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "    print (\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('lstm.h5')\n",
    "    print ('Test score:', score)\n",
    "\n",
    "\n",
    "#训练模型，并保存\n",
    "def train():\n",
    "    print ('Loading Data...')\n",
    "    combined,y=loadfile()\n",
    "    print (len(combined),len(y))\n",
    "    print ('Tokenising...')\n",
    "    combined = tokenizer(combined)\n",
    "    \n",
    "    print ('Training a Word2vec model...')\n",
    "    index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "    \n",
    "    print ('Setting up Arrays for Keras Embedding Layer...')\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)\n",
    "    print (x_train.shape,y_train.shape)\n",
    "    \n",
    "    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def input_transform(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    model=Word2Vec.load('Word2vec_model.pkl')\n",
    "    _,_,combined=create_dictionaries(model,words)\n",
    "    return combined\n",
    "\n",
    "def lstm_predict(string):\n",
    "    '''print ('loading model......')'''\n",
    "    with open('lstm.yml', 'r') as f:\n",
    "        yaml_string = yaml.load(f)\n",
    "    model = model_from_yaml(yaml_string)\n",
    "\n",
    "    '''print ('loading weights......')'''\n",
    "    model.load_weights('lstm.h5')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    data=input_transform(string)\n",
    "    #print (data)\n",
    "    data.reshape(1,-1)\n",
    "    #print(data)\n",
    "    result=model.predict_classes(data)\n",
    "    if result[0][0]==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    #train()\n",
    "\n",
    "    '''\n",
    "    #油耗\n",
    "    s1='1油耗很满意啊 这么重的车 油耗能跑到11左右 已经很省油了 当初买车的时候看论坛说13-15个油 觉得可以接受 ，实际上在市区就11个油，跟紧凑级suv油耗差不多，这个比较让我惊喜。'\n",
    "    s2='1油耗很满意，只有8个有左右，可能我预期10个油有点高。但是我看到网上也有10个油的，可能跟个人驾驶习惯也有关系。 高速路段表现：回老家跑高速测试过，最低只有7.1个油，还试了一下ACC，挺好用的，但是会失去驾驶乐趣'\n",
    "    s3='-1堵车还是挺费油，因为深圳的朋友开车太赶时间，喜欢插队，所以行政一脚油门一脚刹车确实有点费油了，但是错峰出行，路况好的时候，行政还是比较省油的，完全不用考虑油耗了，所以错峰出行才是王道'\n",
    "    s4='-1换个发动机之后提速更快了，油耗也比原来更高，大概在8.5。城市路段常见拥挤和等待红绿灯，怠速期间容易拉高油耗。'\n",
    "    s5='-1目前行驶了15000公里，油耗不是很满意！平均下来百公里在6.8个左右。我老家在山区，路不太好走，这车底盘高，碰撞不着，一加油，就窜出去了！'\n",
    "    s6='1开了1000多公里了，高速油耗5.7。市区道路6.9，可以说很低了，完全能接受。'\n",
    "    '''\n",
    "    #舒适性\n",
    "    s1='-1舒适性就剩吐槽了。超过半小时坐的浑身难受。前排还好点。后排是真受不了。过个减速带，土路啥的那个颠簸。'\n",
    "    s2='1挺好，尤其过减速带感觉挺柔和'\n",
    "    s3='1舒适性还可以，我坐着不错。老人也说没啥问题。能躺则躺在后排）呵呵'\n",
    "    s4='1整体来说还是比较满意的，过减速带什么的一般般。不过座椅做的。挺好的，特别舒适，空调来风快。'\n",
    "    s5='-1底盘比较硬，隔音方面也不算很好，舒适性方面比较差，可能这个级别的车舒适性都不是很好。'\n",
    "    s6='-1驾驶座不能上下调节，是个糟糕的设计！'\n",
    "    \n",
    "    '''\n",
    "    testlist = [s1, s2, s3, s4, s5, s6]\n",
    "    for string in testlist:\n",
    "        print(  lstm_predict(string)  )\n",
    "    '''\n",
    "        \n",
    "    fr = codecs.open('control.txt','r')\n",
    "    \n",
    "    loss1 = 0\n",
    "    total1 = 0;\n",
    "    loss_1 = 0\n",
    "    total_1 = 0\n",
    "    for line in fr:\n",
    "        temp = lstm_predict(line)\n",
    "        print(temp)\n",
    "        if re.match('1',line) is not None:\n",
    "            total1 = total1+1\n",
    "            if temp == -1:\n",
    "                loss1 = loss1+1\n",
    "        if re.match('-1',line) is not None:\n",
    "            total_1 = total_1+1\n",
    "            if temp == 1:\n",
    "                loss_1 = loss_1+1\n",
    "        if (total1+total_1)%50 == 0 and total_1>0 and total1>0:\n",
    "            print(1-loss1/total1, total1)\n",
    "            print(1-loss_1/total_1, total_1)\n",
    "            print(1-(loss1+loss_1)/(total1+total_1), total1+total_1)\n",
    "            \n",
    "    print(1-loss1/total1, total1)\n",
    "    print(1-loss_1/total_1, total_1)\n",
    "    print(1-(loss1+loss_1)/(total1+total_1), total1+total_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
